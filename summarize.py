# -*- coding: utf-8 -*-
"""SUMMARIZATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BLNiH1k5ySQAcNTT24hgxfbe9vTsML88
"""

import tweepy
import re
import string
import os
import csv
from operator import add
from scipy import spatial
import glob
import sys
import numpy as np
from scipy.spatial import distance
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import random
import heapq
from sklearn.metrics.pairwise import cosine_similarity
# from summarization import get_sentences, get_vectors, ranking, beam_search
import nltk
import pandas as pd
import re
import networkx as nx
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

word_embeddings = {}
f = open('/content/drive/My Drive/glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
f.close()

import numpy as np
import pandas as pd
import re
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import glob
from nltk.tokenize import sent_tokenize

# def get_sentences(tag):
def get_sentences():
    # files = glob.glob("../data/tweets/"+tag+"/*")
    # files=open("/content/drive/My Drive/us_elections1.txt","r")
    sentences = []
    # for file in files:
    fd = open("/content/drive/My Drive/us_elections1.txt","r")
    for line in fd:
        sentences.append(line)
    fd.close()
    return sentences

def get_vectors(sentences,model):
    sen_vectors = []
    for sentence in sentences:
        test_data = word_tokenize(sentence.lower())
        v1 = model.infer_vector(test_data)
        sen_vectors.append(v1)
    return sen_vectors

def sentence_tokenize(tweets):
    sentences = []
    for tweet in tweets:
        sentences.append(sent_tokenize(tweet))
    sentences = [y for x in sentences for y in x]
    return sentences

def remove_stopwords(sen):
    sen_new = " ".join([i for i in sen if i not in stop_words])
    return sen_new


def text_processing(sentences):
    clean_sentences = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")
    clean_sentences = [s.lower() for s in clean_sentences]
    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]
    return clean_sentences

def vector_representations(clean_sentences):
    sentence_vectors = []
    for i in clean_sentences:
        if len(i) != 0:
            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)
        else:
            v = np.zeros((100,))
        sentence_vectors.append(v)
    return sentence_vectors

def similarity_matrix(sentence_vectors):
    sim_mat = np.zeros([len(sentence_vectors), len(sentence_vectors)])
    for i in range(len(sentence_vectors)):
        for j in range(len(sentence_vectors)):
            if i != j:
                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]
    return sim_mat

def apply_pagerank(sim_mat):
    nx_graph = nx.DiGraph(sim_mat)
    scores = nx.pagerank(nx_graph)
    return scores

def summary_extraction(scores,sentences,k):
    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)
    for i in range(k):
        print(ranked_sentences[i][1])

sentences = get_sentences()
sentences = sentence_tokenize(sentences)
clean_sentences = text_processing(sentences)
sentence_vectors = vector_representations(clean_sentences)
sim_mat = similarity_matrix(sentence_vectors)
scores = apply_pagerank(sim_mat)
summary_extraction(scores,sentences,5)

"""### **USING SUMY**"""

!pip install sumy

!pip install "tokenizers==0.5.2"

#Plain text parsers since we are parsing through text
from sumy.parsers.plaintext import PlaintextParser

#for tokenization
from sumy.nlp.tokenizers import Tokenizer

import nltk
nltk.download('punkt')
file = "/content/drive/MyDrive/us_elections1.txt" 
parser = PlaintextParser.from_file(file, Tokenizer("english"))
from sumy.summarizers.lex_rank import LexRankSummarizer 
summarizer = LexRankSummarizer()
#Summarize the document with 5 sentences
summary = summarizer(parser.document, 5) 
for sentence in summary:
 print(sentence)


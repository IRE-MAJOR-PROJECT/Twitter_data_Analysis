# -*- coding: utf-8 -*-
"""data_tweeter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPdKmvPxgWbeShQP3YyM2O4VtVCxq6MS
"""

"""### writing into file"""

res=[]
def write_to_file(seen,text2,text1):
    ans=""
    for tweet in seen:
      tweet = re.sub(r'[^\w\s]', '', tweet)
      ans=ans+tweet
    res.append(ans+"|"+text2+"|"+text1)

"""### cleaning of tweets"""

def pre_processing(text1):
    text1= re.sub('&amp;','&',text1)
    text1 = re.sub(r'^RT[\s]+', '', text1, flags=re.MULTILINE) #removes RT
    text1 = re.sub(r'https?:\/\/.*[\r\n]*', '', text1, flags=re.MULTILINE) #remove links
    text1 = re.sub(r'[:]+', '', text1, flags=re.MULTILINE)	
    text1 = re.sub(r'[^\x00-\x7F]+','', text1)
    return text1

"""### Extraction of tweets"""

import tweepy
import re
import string
import os
import csv
import sys
import numpy as np
from nltk.tokenize import word_tokenize
import nltk
import pandas as pd
import re
import time
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = stopwords.words('english')
tweets_folder = "tweets"

#AUTHENTICATION 

consumer_key = 'JSY5rQoW51WZ26SKS1I4gkOOv'
consumer_key_secret = 'hLq7j0sAzeUQSNpCCrAjIQd0tH1xW2Hfg4aMCeYZFUTJVOnA3q'
access_token = '1263057130380316672-TtCi6oHP41sNIPbxqHf5v7GgiQsr2D'
access_token_secret = 'FnG3ShhDyTX3S7ZBcteLaiO5GgHnUI4Di7G0lNeyP3mKY'

auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)


def tweeter_data(hashtag,slang_dictionary):
	set1=set()
	if not os.path.isdir(tweets_folder+hashtag):
		os.mkdir(tweets_folder+hashtag)
	hash_tag_folder = tweets_folder+hashtag
	count=0
	c=0
	seen = []
	for tweet in tweepy.Cursor(api.search, q="#"+hashtag, lang="en", tweet_mode='extended').items(100):   
		text1=tweet.user.location
		if text1 == "":
			 continue
		text1=pre_processing(text1)
		text2=tweet.user.screen_name
		text2=pre_processing(text2)
		if 'in_reply_to_status_id' in dir(tweet):
			if tweet.in_reply_to_status_id != None:
				continue

		if 'retweeted_status' in dir(tweet):
			text=tweet.retweeted_status.full_text
		else:
			text=tweet.full_text		
		try:
			if tweet.entities['media'][0]['type']== 'photo' or tweet.entities['media'][0]['type']=='video':
				continue
		except:
			pass

		#CLEANING OF EXTRACTED TWEETS
		text=pre_processing(text)
		for wr in text:
			if wr in slang_dictionary:
				text= re.sub(r'\b'+wr+r'\b', slang_dictionary[wr],text)
		new_text = ""		
		for i in text.split(): # remove @ and #words, punctuataion
			if not i.startswith('@') and not i.startswith('#') and i not in string.punctuation:
				new_text+=i+" "	
		text = new_text			
		if len(text)>50:
			if text in set1:
				continue
			else:		
				seen.append(text)
				set1.add(text)
				write_to_file(seen,text2,text1)
				seen=[]
				c=c+1
		# time.sleep(1)


if not os.path.isdir(tweets_folder):
	os.mkdir(tweets_folder)		
REQUIRED_TWEETS=30
c=1
slang_dictionary ={}   # DICTIONARY TO REPLACE COMMONLY USED WORDS 
csvfile= open('Slang_Dictionary.csv','r')
reader = csv.reader(csvfile)
for row in reader :
	  slang_dictionary[str(row[0])]= str(row[1])
input_hashtag = input()
tweeter_data(input_hashtag,slang_dictionary)
output = []
for x in res:
		if x not in output:
			output.append(x)
			print(x)



